{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ac3dd-8658-4088-82ef-dba3319b1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import rasterio\n",
    "# import rasterio.mask\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # File paths\n",
    "# shapefile_path = r\"C:\\Users\\magst\\Desktop\\HFT_RERUN\\MULTIHFT_Extract_9_11_2023_wLatLong.shp\"\n",
    "# tiff1_path = r\"C:\\Users\\magst\\Desktop\\HFT_RERUN\\Landfire\\US_105_CC\\us_105cc.tif\"\n",
    "# tiff2_path = r\"C:\\Users\\magst\\Desktop\\HFT_RERUN\\Landfire\\LF2\\US_140_CC\\us_140cc.tif\"\n",
    "\n",
    "# # Step 1: Read the shapefile\n",
    "# shapefile = gpd.read_file(shapefile_path)\n",
    "\n",
    "# # Step 2: Function to extract values from a TIFF based on a shapefile\n",
    "# def extract_raster_values(tiff_path, shapefile):\n",
    "#     with rasterio.open(tiff_path) as src:\n",
    "#         shapes = [feature[\"geometry\"] for feature in shapefile.__geo_interface__[\"features\"]]\n",
    "#         out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "#         out_image = out_image[0]  # Extract the first band\n",
    "#         return out_image.flatten()\n",
    "\n",
    "# # Step 3: Extract values from both TIFFs\n",
    "# values_tiff1 = extract_raster_values(tiff1_path, shapefile)\n",
    "# values_tiff2 = extract_raster_values(tiff2_path, shapefile)\n",
    "\n",
    "# # Remove NaN values from the arrays\n",
    "# mask = ~np.isnan(values_tiff1) & ~np.isnan(values_tiff2)\n",
    "# values_tiff1 = values_tiff1[mask]\n",
    "# values_tiff2 = values_tiff2[mask]\n",
    "\n",
    "# # Step 4: Create a scatter plot comparing the two TIFF datasets\n",
    "# plt.scatter(values_tiff1, values_tiff2, alpha=0.5, edgecolor='k')\n",
    "# plt.xlabel('Values from us_105cc.tif')\n",
    "# plt.ylabel('Values from us_140cc.tif')\n",
    "# plt.title('Scatter Plot of Extracted Values from Two TIFF Files')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19282984-e7a6-484b-b78a-4c1716153e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "#file_path = r\"C:\\Users\\magst\\Desktop\\HFT_RERUN\\ComparisonLF2001_2014_FULL_plus2020_BUTpre2014.txt\"\n",
    "file_path = r\"C:\\Users\\magst\\Desktop\\HFT_RERUN\\ComparisonLF2001_2014_FULL_plus2020_BUTRANDOMPOINTS.txt\"\n",
    "\n",
    "# Reading the text file into a DataFrame\n",
    "df = pd.read_csv(file_path, sep=\",\")#, header=None)  # Assuming it's a tab-separated file without headers\n",
    "\n",
    "#df = df.dropna(subset=['LC22_F40_220_22'])\n",
    "\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a90c9-2ba2-4343-8b42-9c5709ee9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['us_140cbh'], df['us_105cbh'], alpha=0.5, edgecolor='k')\n",
    "plt.xlabel('us_140cc')\n",
    "plt.ylabel('us_105cc')\n",
    "plt.title('Scatter Plot of us_140cbh vs us_105cbh')\n",
    "plt.xlim(0, 20)  # Set x-axis limits\n",
    "plt.ylim(0, 20)  # Set y-axis limits\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3d334-37b4-49a6-abaf-c837e298d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hexbin(df['us_140ch'], df['us_105ch'], gridsize=10, cmap='Blues', mincnt=1)\n",
    "plt.colorbar(label='Density')\n",
    "plt.xlabel('us_140cc')\n",
    "plt.ylabel('us_105cc')\n",
    "plt.title('Density Scatter Plot of us_140cbh vs us_105cbh')\n",
    "plt.xlim(0, 600)  # Set x-axis limits\n",
    "plt.ylim(0, 600)  # Set y-axis limits\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0e673-93f8-4509-929c-497377ff6ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hexbin(df['us_140ch'], df['us_105ch'], gridsize=5000, cmap='Blues', mincnt=1)\n",
    "plt.colorbar(label='Density')\n",
    "plt.xlabel('us_140ch')\n",
    "plt.ylabel('us_105ch')\n",
    "plt.title('Hexbin Plot of us_140ch vs us_105ch with More Hexes')\n",
    "plt.xlim(0, 20)  # Optional: Set x-axis limits\n",
    "plt.ylim(0, 20)  # Optional: Set y-axis limits\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a734263a-0aaf-4e3e-bcc8-ae5e6ab74f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Limit the data to the first 1000 rows\n",
    "df_sample = df.head(1000)\n",
    "\n",
    "# Calculate the point density for the first 1000 points\n",
    "xy = np.vstack([df_sample['us_140cbh'], df_sample['us_105cbh']])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "# Sort the points by density, so the densest points are plotted on top\n",
    "idx = z.argsort()\n",
    "x, y, z = df_sample['us_140cbh'][idx], df_sample['us_105cbh'][idx], z[idx]\n",
    "\n",
    "# Create the scatter plot with color representing the density\n",
    "plt.scatter(x, y, c=z, s=5, edgecolor='black', cmap='viridis', alpha=0.5)\n",
    "plt.xlabel('us_140cc')\n",
    "plt.ylabel('us_105cc')\n",
    "plt.title('Density Scatter Plot of us_140cbh vs us_105cbh (First 1000 Points)')\n",
    "plt.xlim(0, 20)  # Set x-axis limits\n",
    "plt.ylim(0, 20)  # Set y-axis limits\n",
    "plt.colorbar(label='Density')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9f3f4-0fa7-4348-a100-48d2d1e65fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# # File path\n",
    "# file_path = r\"C:\\Users\\magst\\Desktop\\HFT_RERUN\\ComparisonLF2001_2014.txt\"\n",
    "\n",
    "# # Reading the text file into a DataFrame\n",
    "# df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "# Perform Pearson correlation test\n",
    "correlation, p_value = stats.pearsonr(df['us_140cc'], df['us_105cc'])\n",
    "\n",
    "# Output the results\n",
    "print(f\"Pearson Correlation Coefficient: {correlation}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Check for statistical significance\n",
    "alpha = 0.05  # Commonly used significance level\n",
    "if p_value < alpha:\n",
    "    print(\"The relationship between us_140cbh and us_105cbh is statistically significant.\")\n",
    "else:\n",
    "    print(\"The relationship between us_140cbh and us_105cbh is not statistically significant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ced332-0933-4a76-abf4-3c9c21f8a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlation, p_value = pearsonr(df['us_140cbh'], df['us_105cbh'])\n",
    "print(f\"Pearson correlation coefficient: {correlation}\")\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb2cc8-76b0-4e64-b428-1bb20e2bd327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "# Pairs to test\n",
    "# column_pairs = [\n",
    "#     ('us_140cbh_1', 'us_105cbh_1'),\n",
    "#     ('us_140cc_1', 'us_105cc_1'),\n",
    "#     ('us_140ch_1', 'us_105ch_1')\n",
    "# ]\n",
    "column_pairs = [\n",
    "    ('us_140cbh', 'us_105cbh'),\n",
    "    ('us_140cc', 'us_105cc'),\n",
    "    ('us_140ch', 'us_105ch')\n",
    "]\n",
    "\n",
    "# column_pairs = [\n",
    "#     #('us_140cbh_1', 'us_105cbh_1'),\n",
    "#     ('us_105cc', 'LC22_CC_22'),\n",
    "#     ('us_140ch', 'LC22_CH_22'),\n",
    "#     ('us_105cbh', 'LC22_CBH_22'),\n",
    "\n",
    "#     #('us_140ch', 'LC22_CH_220')\n",
    "# ]\n",
    "\n",
    "# Iterate over each pair and perform Pearson correlation\n",
    "for col1, col2 in column_pairs:\n",
    "    correlation, p_value = stats.pearsonr(df[col1], df[col2])\n",
    "    \n",
    "    # Output the results\n",
    "    print(f\"Testing {col1} vs {col2}:\")\n",
    "    print(f\"Pearson Correlation Coefficient: {correlation}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "    \n",
    "    # Check for statistical significance\n",
    "    alpha = 0.01\n",
    "    if p_value < alpha:\n",
    "        print(\"The relationship is statistically significant.\\n\")\n",
    "    else:\n",
    "        print(\"The relationship is not statistically significant.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1ee85-1385-4f26-a16f-e499939c5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Pairs to test\n",
    "# column_pairs = [\n",
    "#     ('us_140cbh', 'us_105cbh'),\n",
    "#     ('us_140cc', 'us_105cc'),\n",
    "#     ('us_140ch', 'us_105ch')\n",
    "# ]\n",
    "\n",
    "# Print Pearson correlation results with higher precision for p-value\n",
    "for col1, col2 in column_pairs:\n",
    "    correlation, p_value = stats.pearsonr(df[col1], df[col2])\n",
    "    \n",
    "    # Output the results with higher precision for p-value\n",
    "    print(f\"Testing {col1} vs {col2}:\")\n",
    "    print(f\"Pearson Correlation Coefficient: {correlation:.20f}\")\n",
    "    print(f\"P-value: {p_value:.10e}\")  # Scientific notation for very small numbers\n",
    "    \n",
    "    # Check for statistical significance\n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        print(\"The relationship is statistically significant.\\n\")\n",
    "    else:\n",
    "        print(\"The relationship is not statistically significant.\\n\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd50660-949e-4bb5-a427-f9d5a034467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394db774-4e33-4915-b29a-1fad4a6ac22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of column pairs\n",
    "column_pairs = [\n",
    "    ('us_140cbh_1', 'us_105cbh_1'),\n",
    "    #('us_140cc_1', 'us_105cc_1'),\n",
    "    #('us_140ch_1', 'us_105ch_1')\n",
    "]\n",
    "\n",
    "# Loop through each pair and plot the histogram of their difference\n",
    "for col1, col2 in column_pairs:\n",
    "    # Subtract one column from the other\n",
    "    diff = df[col1] - df[col2]\n",
    "    \n",
    "    # Plot the histogram of the differences\n",
    "    plt.figure()\n",
    "    plt.hist(diff, bins=100, alpha=0.7, color='grey', edgecolor='black')\n",
    "    plt.xlabel(f'Difference')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of Differences Between CH 2001 and CH 2014')\n",
    "    plt.xlim(-100, 100)  # Set the x-axis limits to -100 and 100\n",
    "    #plt.grid(True)\n",
    "    plt.savefig(f'C:\\\\Users\\\\magst\\\\Desktop\\\\HFT_RERUN\\\\FIGURES\\\\histogramNF_{col1}_vs_{col2}.png')  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28b1ea-5b71-4d4f-8c9d-1a7b5fc3dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of column pairs\n",
    "column_pairs = [\n",
    "    ('us_140cbh_1', 'us_105cbh_1'),\n",
    "    ('us_140cc_1', 'us_105cc_1'),\n",
    "    ('us_140ch_1', 'us_105ch_1')\n",
    "]\n",
    "\n",
    "# Loop through each pair and plot the histogram of their difference, saving each figure\n",
    "for col1, col2 in column_pairs:\n",
    "    diff = df[col1] - df[col2]\n",
    "    plt.figure()\n",
    "    plt.hist(diff, bins=100, alpha=0.7, color='grey', edgecolor='black')\n",
    "    plt.xlabel(f'{col1} - {col2}')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Histogram of {col1} - {col2}')\n",
    "    plt.xlim(-100, 100)\n",
    "    plt.savefig(f'histogram_{col1}_vs_{col2}.png')  # Save the figure\n",
    "    plt.close()  # Close the figure to save memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35596cc-3882-45c0-b47a-53c3829dbda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64edde9-9256-4a48-89c0-0f6199f748d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d3349-be71-43cf-bb83-4c741edfefc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# Create a contingency table (cross-tabulation of the two columns)\n",
    "#contingency_table = pd.crosstab(df['us_105fbfm'], df['us_140fbfm'])\n",
    "#contingency_table = pd.crosstab(df['us_105evt'], df['us_140evt'])\n",
    "contingency_table = pd.crosstab(df['us_105fbfm40_1'], df['us_140fbfm40_1'])\n",
    "#contingency_table = pd.crosstab(df['us_105evt'], df['us_140evt'])\n",
    "\n",
    "# Perform the Chi-Square test\n",
    "chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Chi-Square Statistic: {chi2}\")\n",
    "print(f\"P-value: {p_value:.10f}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")\n",
    "print(\"Expected frequencies:\")\n",
    "print(expected)\n",
    "\n",
    "# Interpret the p-value\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant association between the two categorical variables.\")\n",
    "else:\n",
    "    print(\"There is no significant association between the two categorical variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3fd9c0-0cb4-40e7-a2ed-23a27befe237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def cramers_v(contingency_table):\n",
    "    \"\"\"Calculate Cramér's V for a contingency table.\"\"\"\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    n = contingency_table.sum().sum()\n",
    "    r, k = contingency_table.shape\n",
    "    cramers_v_value = np.sqrt((chi2 / n) / (min(k - 1, r - 1)))\n",
    "    return cramers_v_value\n",
    "\n",
    "# Create the contingency table for the two categorical variables\n",
    "contingency_table = pd.crosstab(df['us_105fbfm40_1'], df['us_140fbfm40_1'])\n",
    "\n",
    "# Create a square matrix for the categorical columns\n",
    "categories_1 = df['us_105fbfm40_1'].unique()\n",
    "categories_2 = df['us_140fbfm40_1'].unique()\n",
    "\n",
    "# Initialize a matrix to hold the Cramér's V values\n",
    "matrix = pd.DataFrame(np.zeros((len(categories_1), len(categories_2))), index=categories_1, columns=categories_2)\n",
    "\n",
    "# Fill in the Cramér's V values for each pair\n",
    "for i in matrix.index:\n",
    "    for j in matrix.columns:\n",
    "        sub_contingency_table = pd.crosstab(df[df['us_105fbfm40_1'] == i]['us_105fbfm40_1'], df[df['us_140fbfm40_1'] == j]['us_140fbfm40_1'])\n",
    "        matrix.loc[i, j] = cramers_v(sub_contingency_table)\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matrix, annot=True, cmap='coolwarm', cbar=True)\n",
    "plt.title(\"Cramér's V Correlation Matrix for Categorical Variables\")\n",
    "plt.xlabel(\"us_140fbfm40\")\n",
    "plt.ylabel(\"us_105fbfm40\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d088b-25d4-4bce-8017-86ec9368ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Generate the contingency table\n",
    "contingency_table = pd.crosstab(df['us_105fbfm40_1'], df['us_105fbfm40_1'])\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(contingency_table, annot=False, cmap=\"viridis_r\", linewidths=0.5)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('us_140fbfm40_1')\n",
    "plt.ylabel('us_105fbfm40_1')\n",
    "plt.title('Heatmap of Contingency Table: us_105fbfm40_1 vs us_140fbfm40_1')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9456d-79ec-4131-9fa7-c91a9673f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure df exists\n",
    "# Sample unique categories for each column and append one of each to the original dataframe\n",
    "unique_us_105 = df['us_140fbfm40_1'].unique()\n",
    "unique_us_140 = df['us_140fbfm40_1'].unique()\n",
    "\n",
    "# Create a DataFrame that contains one sample from each unique category\n",
    "guaranteed_samples = pd.DataFrame({\n",
    "    'us_105fbfm40_1': unique_us_105,\n",
    "    'us_140fbfm40_1': unique_us_140[:len(unique_us_105)]  # Ensures they match in length\n",
    "})\n",
    "\n",
    "# Append guaranteed samples to the existing dataframe\n",
    "df_with_guarantees = pd.concat([df, guaranteed_samples], ignore_index=True)\n",
    "\n",
    "# Generate the contingency table from the updated DataFrame\n",
    "contingency_table = pd.crosstab(df_with_guarantees['us_105fbfm40_1'], df_with_guarantees['us_140fbfm40_1'])\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(contingency_table, annot=False, cmap=\"viridis_r\", linewidths=0.1)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('FBFM40 2014')\n",
    "plt.ylabel('FBFM40 2001')\n",
    "plt.title('Heatmap of FBFM40 in 2014 vs FMFB40 in 2001')\n",
    "\n",
    "plt.savefig(r'C:\\Users\\magst\\Desktop\\HFT_RERUN\\FIGURES\\heatmap_contingency_table_FBFM_NF.png', dpi=300, bbox_inches='tight')  # Saves the figure\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43cc0e-e39d-41e7-b555-30fd8f373b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1096e35-b7c6-40e1-94f0-0ef36580eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure df exists\n",
    "# Sample unique categories for each column and append one of each to the original dataframe\n",
    "unique_us_105 = df['us_105evt_1'].unique()\n",
    "unique_us_140 = df['us_140evt_1'].unique()\n",
    "\n",
    "# Create a DataFrame that contains one sample from each unique category\n",
    "guaranteed_samples = pd.DataFrame({\n",
    "    'us_105evt_1': unique_us_105,\n",
    "    'us_140evt_1': unique_us_140[:len(unique_us_105)]  # Ensures they match in length\n",
    "})\n",
    "\n",
    "# Append guaranteed samples to the existing dataframe\n",
    "df_with_guarantees = pd.concat([df, guaranteed_samples], ignore_index=True)\n",
    "\n",
    "# Generate the contingency table from the updated DataFrame\n",
    "contingency_table = pd.crosstab(df_with_guarantees['us_105evt_1'], df_with_guarantees['us_140evt_1'])\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(contingency_table, annot=False, cmap=\"viridis_r\", linewidths=0.1)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('EVT 2014')\n",
    "plt.ylabel('EVT 2001')\n",
    "plt.title('Heatmap of EVT in 2014 vs EVT in 2001')\n",
    "\n",
    "#plt.savefig(r'C:\\Users\\magst\\Desktop\\HFT_RERUN\\FIGURES\\heatmap_contingency_table_EVT_NF.png', dpi=300, bbox_inches='tight')  # Saves the figure\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf7415-9847-4dfe-be0e-5e8f4da7107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "#file_path = r\"C:\\Users\\magst\\Desktop\\HFT_RERUN\\ComparisonLF2001_2014_FULL_plus2020_BUTpre2014.txt\"\n",
    "file_path = r\"C:\\Users\\magst\\Desktop\\HFT_RERUN\\ComparisonLF2001_2014_FULL_plus2020_BUTRANDOMPOINTS.txt\"\n",
    "\n",
    "# Reading the text file into a DataFrame\n",
    "df = pd.read_csv(file_path, sep=\",\")#, header=None)  # Assuming it's a tab-separated file without headers\n",
    "\n",
    "#df = df.dropna(subset=['LC22_F40_220_22'])\n",
    "\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60636b-75db-4e3b-8b2d-7426cb687037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Example mapping of EVT codes to NVCSorder (you can modify with your actual data)\n",
    "evt_to_nvcsorder = {\n",
    "    11: \"Non-vegetated\",\n",
    "    12: \"Non-vegetated\",\n",
    "    13: \"Open tree canopy\",\n",
    "    14: \"Open tree canopy\",\n",
    "    15: \"Open tree canopy\",\n",
    "    16: \"Herbaceous - grassland\",\n",
    "    17: \"Shrubland\",\n",
    "    23: \"No Dominant Lifeform\",\n",
    "    24: \"No Dominant Lifeform\",\n",
    "    25: \"No Dominant Lifeform\",\n",
    "    31: \"Non-vegetated\",\n",
    "    32: \"Non-vegetated\",\n",
    "    60: \"Open tree canopy\",\n",
    "    63: \"Herbaceous - grassland\",\n",
    "    64: \"Herbaceous - grassland\",\n",
    "    65: \"Herbaceous - grassland\",\n",
    "    66: \"Herbaceous - grassland\",\n",
    "    75: \"Herbaceous - grassland\",\n",
    "    76: \"Herbaceous - grassland\",\n",
    "    78: \"Herbaceous - grassland\",\n",
    "    81: \"Herbaceous - grassland\",\n",
    "    82: \"Herbaceous - grassland\",\n",
    "    95: \"Herbaceous - grassland\",\n",
    "    2001: \"Sparsely Vegetated\",\n",
    "    2006: \"Sparsely Vegetated\",\n",
    "    2007: \"Sparsely Vegetated\",\n",
    "    2011: \"Open tree canopy\",\n",
    "    2012: \"Open tree canopy\",\n",
    "    2016: \"Open tree canopy\",\n",
    "    2024: \"Open tree canopy\",\n",
    "    2025: \"Open tree canopy\",\n",
    "    2046: \"Open tree canopy\",\n",
    "    2049: \"Open tree canopy\",\n",
    "    2050: \"Closed tree canopy\",\n",
    "    2051: \"Closed tree canopy\",\n",
    "    2052: \"Closed tree canopy\",\n",
    "    2054: \"Open tree canopy\",\n",
    "    2055: \"Closed tree canopy\",\n",
    "    2056: \"Closed tree canopy\",\n",
    "    2057: \"Open tree canopy\",\n",
    "    2059: \"Open tree canopy\",\n",
    "    2061: \"Open tree canopy\",\n",
    "    2062: \"Open tree canopy\",\n",
    "    2064: \"Dwarf-shrubland\",\n",
    "    2066: \"Dwarf-shrubland\",\n",
    "    2070: \"Dwarf-shrubland\",\n",
    "    2072: \"Dwarf-shrubland\",\n",
    "    2078: \"Shrubland\",\n",
    "    2080: \"Shrubland\",\n",
    "    2081: \"Shrubland\",\n",
    "    2085: \"Shrubland\",\n",
    "    2086: \"Shrubland\",\n",
    "    2093: \"Shrubland\",\n",
    "    2094: \"Shrubland\",\n",
    "    2095: \"Shrubland\",\n",
    "    2103: \"Shrubland\",\n",
    "    2104: \"Shrubland\",\n",
    "    2106: \"Shrubland\",\n",
    "    2107: \"Shrubland\",\n",
    "    2115: \"Sparse tree canopy\",\n",
    "    2117: \"Sparse tree canopy\",\n",
    "    2119: \"Sparse tree canopy\",\n",
    "    2121: \"Herbaceous - shrub-steppe\",\n",
    "    2125: \"Herbaceous - shrub-steppe\",\n",
    "    2126: \"Herbaceous - shrub-steppe\",\n",
    "    2127: \"Herbaceous - shrub-steppe\",\n",
    "    2132: \"Herbaceous - grassland\",\n",
    "    2133: \"Herbaceous - grassland\",\n",
    "    2135: \"Herbaceous - grassland\",\n",
    "    2139: \"Herbaceous - grassland\",\n",
    "    2140: \"Herbaceous - grassland\",\n",
    "    2141: \"Herbaceous - grassland\",\n",
    "    2143: \"Herbaceous - grassland\",\n",
    "    2144: \"Herbaceous - grassland\",\n",
    "    2145: \"Herbaceous - grassland\",\n",
    "    2146: \"Herbaceous - grassland\",\n",
    "    2147: \"Herbaceous - grassland\",\n",
    "    2148: \"Herbaceous - grassland\",\n",
    "    2149: \"Herbaceous - grassland\",\n",
    "    2153: \"Shrubland\",\n",
    "    2154: \"Open tree canopy\",\n",
    "    2155: \"Open tree canopy\",\n",
    "    2159: \"Open tree canopy\",\n",
    "    2160: \"Shrubland\",\n",
    "    2162: \"Open tree canopy\",\n",
    "    2166: \"Closed tree canopy\",\n",
    "    2167: \"Closed tree canopy\",\n",
    "    2169: \"Deciduous shrubland\",\n",
    "    2179: \"Open tree canopy\",\n",
    "    2180: \"Open tree canopy\",\n",
    "    2181: \"Herbaceous - grassland\",\n",
    "    2182: \"Herbaceous - grassland\",\n",
    "    2183: \"Herbaceous - grassland\",\n",
    "    2208: \"Closed tree canopy\",\n",
    "    2210: \"Shrubland\",\n",
    "    2211: \"Shrubland\",\n",
    "    2213: \"Shrubland\",\n",
    "    2214: \"Shrubland\",\n",
    "    2215: \"Shrubland\",\n",
    "    2217: \"Shrubland\",\n",
    "    2220: \"Shrubland\",\n",
    "    2385: \"Open tree canopy\",\n",
    "    2495: \"Herbaceous - grassland\",\n",
    "    2503: \"Herbaceous - grassland\",\n",
    "    2504: \"Herbaceous - grassland\"\n",
    "}\n",
    "\n",
    "# Replace EVT codes with corresponding NVCSorder in the DataFrame\n",
    "df['us_105evt_1'] = df['us_105evt_1'].map(evt_to_nvcsorder)\n",
    "\n",
    "\n",
    "evt_to_nvcsorder_140 = {\n",
    "    3001: \"Sparsely Vegetated\",\n",
    "    3006: \"Sparsely Vegetated\",\n",
    "    3007: \"Sparsely Vegetated\",\n",
    "    3011: \"Open tree canopy\",\n",
    "    3012: \"Open tree canopy\",\n",
    "    3016: \"Open tree canopy\",\n",
    "    3019: \"Open tree canopy\",\n",
    "    3024: \"Open tree canopy\",\n",
    "    3025: \"Open tree canopy\",\n",
    "    3046: \"Open tree canopy\",\n",
    "    3049: \"Open tree canopy\",\n",
    "    3050: \"Closed tree canopy\",\n",
    "    3051: \"Closed tree canopy\",\n",
    "    3052: \"Closed tree canopy\",\n",
    "    3054: \"Open tree canopy\",\n",
    "    3055: \"Closed tree canopy\",\n",
    "    3056: \"Closed tree canopy\",\n",
    "    3057: \"Open tree canopy\",\n",
    "    3059: \"Open tree canopy\",\n",
    "    3061: \"Open tree canopy\",\n",
    "    3062: \"Open tree canopy\",\n",
    "    3064: \"Dwarf-shrubland\",\n",
    "    3066: \"Dwarf-shrubland\",\n",
    "    3070: \"Dwarf-shrubland\",\n",
    "    3072: \"Dwarf-shrubland\",\n",
    "    3078: \"Shrubland\",\n",
    "    3080: \"Shrubland\",\n",
    "    3081: \"Shrubland\",\n",
    "    3085: \"Shrubland\",\n",
    "    3086: \"Shrubland\",\n",
    "    3093: \"Shrubland\",\n",
    "    3095: \"Shrubland\",\n",
    "    3103: \"Shrubland\",\n",
    "    3104: \"Shrubland\",\n",
    "    3106: \"Shrubland\",\n",
    "    3107: \"Shrubland\",\n",
    "    3115: \"Sparse tree canopy\",\n",
    "    3117: \"Sparse tree canopy\",\n",
    "    3119: \"Sparse tree canopy\",\n",
    "    3125: \"Herbaceous - shrub-steppe\",\n",
    "    3126: \"Herbaceous - shrub-steppe\",\n",
    "    3127: \"Herbaceous - shrub-steppe\",\n",
    "    3132: \"Herbaceous - grassland\",\n",
    "    3133: \"Herbaceous - grassland\",\n",
    "    3135: \"Herbaceous - grassland\",\n",
    "    3139: \"Herbaceous - grassland\",\n",
    "    3140: \"Herbaceous - grassland\",\n",
    "    3141: \"Herbaceous - grassland\",\n",
    "    3143: \"Herbaceous - grassland\",\n",
    "    3144: \"Herbaceous - grassland\",\n",
    "    3145: \"Herbaceous - grassland\",\n",
    "    3146: \"Herbaceous - grassland\",\n",
    "    3147: \"Herbaceous - grassland\",\n",
    "    3148: \"Herbaceous - grassland\",\n",
    "    3149: \"Herbaceous - grassland\",\n",
    "    3151: \"Riparian\",\n",
    "    3153: \"Shrubland\",\n",
    "    3154: \"Open tree canopy\",\n",
    "    3155: \"Open tree canopy\",\n",
    "    3159: \"Open tree canopy\",\n",
    "    3160: \"Shrubland\",\n",
    "    3162: \"Open tree canopy\",\n",
    "    3166: \"Closed tree canopy\",\n",
    "    3167: \"Closed tree canopy\",\n",
    "    3169: \"Deciduous shrubland\",\n",
    "    3180: \"Open tree canopy\",\n",
    "    3181: \"Herbaceous - grassland\",\n",
    "    3182: \"Herbaceous - grassland\",\n",
    "    3183: \"Herbaceous - grassland\",\n",
    "    3191: \"Herbaceous - grassland\",\n",
    "    3195: \"Herbaceous - grassland\",\n",
    "    3208: \"Closed tree canopy\",\n",
    "    3210: \"Shrubland\",\n",
    "    3211: \"Shrubland\",\n",
    "    3212: \"Shrubland\",\n",
    "    3213: \"Shrubland\",\n",
    "    3214: \"Shrubland\",\n",
    "    3215: \"Shrubland\",\n",
    "    3217: \"Shrubland\",\n",
    "    3218: \"Sparsely Vegetated\",\n",
    "    3219: \"Sparsely Vegetated\",\n",
    "    3220: \"Shrubland\",\n",
    "    3222: \"Sparsely Vegetated\",\n",
    "    3250: \"Shrubland\",\n",
    "    3251: \"Open tree canopy\",\n",
    "    3252: \"Shrubland\",\n",
    "    3253: \"Open tree canopy\",\n",
    "    3254: \"Herbaceous - grassland\",\n",
    "    3255: \"Open tree canopy\",\n",
    "    3256: \"Herbaceous - shrub-steppe\",\n",
    "    3259: \"Shrubland\",\n",
    "    3292: \"Non-vegetated\",\n",
    "    3293: \"Non-vegetated\",\n",
    "    3294: \"Non-vegetated\",\n",
    "    3295: \"Non-vegetated\",\n",
    "    3296: \"Developed\",\n",
    "    3297: \"Developed\",\n",
    "    3298: \"Developed\",\n",
    "    3299: \"Developed\",\n",
    "    3385: \"Open tree canopy\",\n",
    "    3495: \"Herbaceous - grassland\",\n",
    "    3503: \"Herbaceous - grassland\",\n",
    "    3504: \"Herbaceous - grassland\",\n",
    "    3900: \"Open tree canopy\",\n",
    "    3901: \"Open tree canopy\",\n",
    "    3902: \"Open tree canopy\",\n",
    "    3903: \"Herbaceous - grassland\",\n",
    "    3904: \"Shrubland\",\n",
    "    3911: \"Open tree canopy\",\n",
    "    3914: \"Shrubland\",\n",
    "    3920: \"Open tree canopy\",\n",
    "    3921: \"Open tree canopy\",\n",
    "    3922: \"Open tree canopy\",\n",
    "    3923: \"Shrubland\",\n",
    "    3924: \"Herbaceous - grassland\",\n",
    "    3926: \"Open tree canopy\",\n",
    "    3928: \"Shrubland\",\n",
    "    3929: \"Herbaceous - grassland\",\n",
    "    3940: \"Open tree canopy\",\n",
    "    3941: \"Open tree canopy\",\n",
    "    3943: \"Shrubland\",\n",
    "    3944: \"Herbaceous - grassland\",\n",
    "    3949: \"Herbaceous - grassland\",\n",
    "    3960: \"Open tree canopy\",\n",
    "    3961: \"Shrubland\",\n",
    "    3963: \"Herbaceous - grassland\",\n",
    "    3964: \"Herbaceous - grassland\",\n",
    "    3965: \"Herbaceous - grassland\",\n",
    "    3966: \"Herbaceous - grassland\",\n",
    "    3967: \"Herbaceous - grassland\",\n",
    "    3968: \"Herbaceous - grassland\",\n",
    "    3984: \"Herbaceous - grassland\",\n",
    "    3985: \"Herbaceous - grassland\",\n",
    "    3986: \"Herbaceous - grassland\",\n",
    "    3987: \"Herbaceous - grassland\",\n",
    "    3988: \"Herbaceous - grassland\",\n",
    "}\n",
    "\n",
    "\n",
    "df['us_140evt_1'] = df['us_140evt_1'].map(evt_to_nvcsorder_140)\n",
    "\n",
    "# Get unique values from both columns\n",
    "unique_us_105 = pd.unique(df['us_105evt_1'].dropna())\n",
    "unique_us_140 = pd.unique(df['us_140evt_1'].dropna())\n",
    "\n",
    "# Find the missing categories in each column\n",
    "missing_in_us_140 = [cat for cat in unique_us_105 if cat not in unique_us_140]\n",
    "missing_in_us_105 = [cat for cat in unique_us_140 if cat not in unique_us_105]\n",
    "\n",
    "# Create DataFrame to add missing categories to each column\n",
    "missing_pairs = pd.DataFrame({\n",
    "    'us_105evt_1': missing_in_us_105 + [None] * len(missing_in_us_140),  # Add None for alignment\n",
    "    'us_140evt_1': missing_in_us_140 + [None] * len(missing_in_us_105)  # Add None for alignment\n",
    "})\n",
    "\n",
    "# Append the missing categories to the existing DataFrame\n",
    "df = pd.concat([df, missing_pairs], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Generate the contingency table using NVCSorder\n",
    "contingency_table = pd.crosstab(df['us_105evt_1'], df['us_140evt_1'])\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(contingency_table, annot=False, cmap=\"viridis_r\", linewidths=0.1)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('EVT 2014')\n",
    "plt.ylabel('EVT 2001')\n",
    "plt.title('Heatmap of EVT in 2014 vs EVT in 2001')\n",
    "plt.savefig(r'C:\\Users\\magst\\Desktop\\HFT_RERUN\\FIGURES\\heatmap_contingency_table_EVT_NF.png', dpi=300, bbox_inches='tight')  # Saves the figure\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eaad57-a23e-405f-ba62-23608c9a832d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3032499-d03e-4979-8b27-8743fb917942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure df exists\n",
    "# Sample unique categories for each column and append one of each to the original dataframe\n",
    "unique_us_105 = df['us_140fbfm'].unique()\n",
    "unique_us_140 = df['us_140fbfm'].unique()\n",
    "\n",
    "# Create a DataFrame that contains one sample from each unique category\n",
    "guaranteed_samples = pd.DataFrame({\n",
    "    'us_105fbfm40': unique_us_105,\n",
    "    'us_140fbfm40': unique_us_140[:len(unique_us_105)]  # Ensures they match in length\n",
    "})\n",
    "\n",
    "# Append guaranteed samples to the existing dataframe\n",
    "df_with_guarantees = pd.concat([df, guaranteed_samples], ignore_index=True)\n",
    "\n",
    "# Generate the contingency table from the updated DataFrame\n",
    "contingency_table = pd.crosstab(df_with_guarantees['us_105fbfm'], df_with_guarantees['us_140fbfm'])\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(contingency_table, annot=False, cmap=\"viridis_r\", linewidths=0.1)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('FBFM40 2014')\n",
    "plt.ylabel('FBFM40 2001')\n",
    "plt.title('Heatmap of FBFM40 in 2014 vs FMFB40 in 2001')\n",
    "\n",
    "#plt.savefig(r'C:\\Users\\magst\\Desktop\\HFT_RERUN\\FIGURES\\heatmap_contingency_table_FBFM_NF.png', dpi=300, bbox_inches='tight')  # Saves the figure\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a47e0a4-d818-4929-b9d0-f87e9a15fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"C:\\\\Users\\\\magst\\\\Desktop\\\\HFT_RERUN\\\\MULTIHFT_Extract_9_11_2023_wLatLong.txt\"\n",
    "\n",
    "# Read the file into a DataFrame\n",
    "df = pd.read_csv(file_path, delimiter=',')\n",
    "\n",
    "# Print the DataFrame\n",
    "#print(df)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a347f-79dd-4366-ae3b-b5d90410500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot for WHP vs BP\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(df['BP_12_13_1'], df['FLEP8_12_1'], alpha=0.5, s=1)\n",
    "plt.title('WHP vs BP')\n",
    "plt.xlabel('WHP_12_13_')\n",
    "plt.ylabel('BP_12_13_1')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d29399-c047-4a1c-927d-ea809422ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create a 3D figure\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# 3D scatter plot\n",
    "ax.scatter(df['WHP_12_13_'], df['BP_12_13_1'], df['FLEP8_12_1'], alpha=0.5, s=5)\n",
    "\n",
    "# Set labels\n",
    "ax.set_title('3D Scatter Plot: WHP vs BP vs FLEP8')\n",
    "ax.set_xlabel('WHP_12_13_')\n",
    "ax.set_ylabel('BP_12_13_1')\n",
    "ax.set_zlabel('FLEP8_12_1')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d3733-7f28-48c8-907d-486542f1d500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a820ff-3999-4e36-8e75-f92822e87176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\S_USA.Activity_HazFuelTrt_PL\\S_USA.Activity_HazFuelTrt_PL.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e257dc5-20b0-47c0-ad74-1627e9f2a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # THE EPIC RERUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffebb1b9-324f-4277-a6ac-58dcfbf26ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the shapefile\n",
    "shapefile_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\HazFuelTrt_PL_CO_NEWCRS.shp\"\n",
    "\n",
    "# Read the shapefile using GeoPandas\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Plot the shapefile\n",
    "gdf.plot(figsize=(10, 10), color='lightblue', edgecolor='black')\n",
    "\n",
    "# Add plot title and display the plot\n",
    "plt.title(\"Hazardous Fuel Treatments - Colorado\")\n",
    "plt.show()\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d54ce-b9f9-4f99-8821-e42839dd728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKS BINARY\n",
    "\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the shapefile\n",
    "gdf = gpd.read_file(r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\HazFuelTrt_PL_CO_NEWCRS.shp\")\n",
    "\n",
    "# Step 2: Filter only valid geometries and by ACTIVITY_C == '1120'\n",
    "valid_gdf = gdf[gdf.is_valid]  # Filter valid geometries\n",
    "filtered_gdf = valid_gdf[valid_gdf['ACTIVITY_C'] == '1120']  # Filter by ACTIVITY_C\n",
    "\n",
    "# Step 3: Ensure there are valid geometries after filtering\n",
    "if filtered_gdf.empty or filtered_gdf.geometry.is_empty.all():\n",
    "    raise ValueError(\"No valid geometries found in the filtered GeoDataFrame.\")\n",
    "\n",
    "# Step 4: Load the reference raster to match extent, resolution, and CRS\n",
    "reference_raster_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\WRC2020\\CO\\BP_CO.tif\"\n",
    "with rasterio.open(reference_raster_path) as ref_src:\n",
    "    transform = ref_src.transform  # Get the transform of the reference raster\n",
    "    ncols, nrows = ref_src.width, ref_src.height  # Get the width and height\n",
    "    crs = ref_src.crs  # Get the CRS\n",
    "    out_meta = ref_src.meta.copy()  # Copy the reference raster's metadata\n",
    "\n",
    "# Step 5: Rasterize the filtered GeoDataFrame to match the reference raster's extent\n",
    "shapes = ((geom, 1) for geom in filtered_gdf.geometry)  # 1 if the polygon is present\n",
    "raster = rasterize(\n",
    "    shapes,\n",
    "    out_shape=(nrows, ncols),  # Match the shape of the reference raster\n",
    "    transform=transform,  # Use the same transform as the reference raster\n",
    "    fill=0,  # 0 where no polygon\n",
    "    dtype='uint8'\n",
    ")\n",
    "\n",
    "# Step 6: Update metadata for the new raster, set nodata value to 255\n",
    "out_meta.update({\n",
    "    'driver': 'GTiff',\n",
    "    'height': nrows,\n",
    "    'width': ncols,\n",
    "    'transform': transform,\n",
    "    'crs': crs,\n",
    "    'dtype': 'uint8',\n",
    "    'nodata': 255  # Valid nodata value for uint8\n",
    "})\n",
    "\n",
    "# Step 7: Save the raster to a file\n",
    "output_raster_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\Activity_1120_30m_snapped.tif\"\n",
    "with rasterio.open(output_raster_path, 'w', **out_meta) as out_raster:\n",
    "    out_raster.write(raster, 1)\n",
    "\n",
    "print(f\"Raster saved to: {output_raster_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b4dfd-a78a-4d4d-86e9-0ee958920f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b1702-2012-46d2-8851-c8517f886d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the shapefile\n",
    "gdf = gpd.read_file(r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\HazFuelTrt_PL_CO_NEWCRS.shp\")\n",
    "\n",
    "# Step 2: Filter only valid geometries and by ACTIVITY_C == '1120'\n",
    "valid_gdf = gdf[gdf.is_valid]  # Filter valid geometries\n",
    "filtered_gdf = valid_gdf[valid_gdf['ACTIVITY_C'] == '1120']  # Filter by ACTIVITY_C\n",
    "\n",
    "# Step 3: Ensure there are valid geometries after filtering\n",
    "if filtered_gdf.empty or filtered_gdf.geometry.is_empty.all():\n",
    "    raise ValueError(\"No valid geometries found in the filtered GeoDataFrame.\")\n",
    "\n",
    "# Step 4: Load the reference raster to match extent, resolution, and CRS\n",
    "reference_raster_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\WRC2020\\CO\\BP_CO.tif\"\n",
    "with rasterio.open(reference_raster_path) as ref_src:\n",
    "    transform = ref_src.transform  # Get the transform of the reference raster\n",
    "    ncols, nrows = ref_src.width, ref_src.height  # Get the width and height\n",
    "    crs = ref_src.crs  # Get the CRS\n",
    "    out_meta = ref_src.meta.copy()  # Copy the reference raster's metadata\n",
    "\n",
    "# Step 5: Rasterize the filtered GeoDataFrame using FY_PLANNED values\n",
    "# Convert geometries and their FY_PLANNED values into a format that rasterize can use\n",
    "shapes = ((geom, value) for geom, value in zip(filtered_gdf.geometry, filtered_gdf['FY_PLANNED']))\n",
    "\n",
    "raster = rasterize(\n",
    "    shapes,\n",
    "    out_shape=(nrows, ncols),  # Match the shape of the reference raster\n",
    "    transform=transform,  # Use the same transform as the reference raster\n",
    "    fill=0,  # 0 where no polygon\n",
    "    dtype='uint16'  # Use uint16 to store year values (FY_PLANNED)\n",
    ")\n",
    "\n",
    "# Step 6: Update metadata for the new raster, set nodata value to 65535\n",
    "out_meta.update({\n",
    "    'driver': 'GTiff',\n",
    "    'height': nrows,\n",
    "    'width': ncols,\n",
    "    'transform': transform,\n",
    "    'crs': crs,\n",
    "    'dtype': 'uint16',  # Using uint16 to accommodate year values\n",
    "    'nodata': 65535  # Valid nodata value for uint16\n",
    "})\n",
    "\n",
    "# Step 7: Save the raster to a file\n",
    "output_raster_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\Activity_1120_30m_snapped_fy_planned.tif\"\n",
    "with rasterio.open(output_raster_path, 'w', **out_meta) as out_raster:\n",
    "    out_raster.write(raster, 1)\n",
    "\n",
    "print(f\"Raster saved to: {output_raster_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc2417-c541-4392-88ec-35d44ab0bcf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603a3d5-9378-4b03-b38b-29af7d02121a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81a59f-0d9c-4bd2-a287-5e3fc9eec5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223b98a-3651-4d28-8c68-890848ba155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single polygon by combining all the polygons using unary_union\n",
    "combined_polygon = gdf.unary_union\n",
    "\n",
    "# Create a new GeoDataFrame to store the combined polygon\n",
    "combined_gdf = gpd.GeoDataFrame(geometry=[combined_polygon], crs=gdf.crs)\n",
    "\n",
    "# Export the combined polygon to a new shapefile\n",
    "output_shapefile_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\HazFuelTrt_PL_CO_combined.shp\"\n",
    "combined_gdf.to_file(output_shapefile_path)\n",
    "\n",
    "print(f\"Combined shapefile exported to: {output_shapefile_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f82fce-bf65-4f84-a78b-0330ff0ec622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the shapefile\n",
    "gdf = gpd.read_file(r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\HazFuelTrt_PL_CO_combined_NEWCRS.shp\")\n",
    "\n",
    "# Step 2: Filter only valid geometries and by ACTIVITY_C == '1120'\n",
    "#valid_gdf = gdf[gdf.is_valid]  # Filter valid geometries\n",
    "#filtered_gdf = valid_gdf[valid_gdf['ACTIVITY_C'] == '1120']  # Filter by ACTIVITY_C\n",
    "filtered_gdf = gdf\n",
    "\n",
    "# Step 3: Ensure there are valid geometries after filtering\n",
    "if filtered_gdf.empty or filtered_gdf.geometry.is_empty.all():\n",
    "    raise ValueError(\"No valid geometries found in the filtered GeoDataFrame.\")\n",
    "\n",
    "# Step 4: Load the reference raster to match extent, resolution, and CRS\n",
    "reference_raster_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\WRC2020\\CO\\BP_CO.tif\"\n",
    "with rasterio.open(reference_raster_path) as ref_src:\n",
    "    transform = ref_src.transform  # Get the transform of the reference raster\n",
    "    ncols, nrows = ref_src.width, ref_src.height  # Get the width and height\n",
    "    crs = ref_src.crs  # Get the CRS\n",
    "    out_meta = ref_src.meta.copy()  # Copy the reference raster's metadata\n",
    "\n",
    "# Step 5: Rasterize the filtered GeoDataFrame to match the reference raster's extent\n",
    "shapes = ((geom, 1) for geom in filtered_gdf.geometry)  # 1 if the polygon is present\n",
    "raster = rasterize(\n",
    "    shapes,\n",
    "    out_shape=(nrows, ncols),  # Match the shape of the reference raster\n",
    "    transform=transform,  # Use the same transform as the reference raster\n",
    "    fill=0,  # 0 where no polygon\n",
    "    dtype='uint8'\n",
    ")\n",
    "\n",
    "# Step 6: Update metadata for the new raster, set nodata value to 255\n",
    "out_meta.update({\n",
    "    'driver': 'GTiff',\n",
    "    'height': nrows,\n",
    "    'width': ncols,\n",
    "    'transform': transform,\n",
    "    'crs': crs,\n",
    "    'dtype': 'uint8',\n",
    "    'nodata': 255  # Valid nodata value for uint8\n",
    "})\n",
    "\n",
    "# Step 7: Save the raster to a file\n",
    "output_raster_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\COMBINED_30m_snapped.tif\"\n",
    "with rasterio.open(output_raster_path, 'w', **out_meta) as out_raster:\n",
    "    out_raster.write(raster, 1)\n",
    "\n",
    "print(f\"Raster saved to: {output_raster_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4a07c-3dc5-4e38-88ed-81f071b37222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a9d5c-d58c-4495-a492-48dc5d97358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Step 1: Load the raster\n",
    "raster_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\COMBINED_30m_snapped.tif\"\n",
    "with rasterio.open(raster_path) as src:\n",
    "    raster_data = src.read(1)  # Read the first band\n",
    "    transform = src.transform  # Get the transform\n",
    "    crs = src.crs  # Get the CRS\n",
    "\n",
    "# Step 2: Find the indices of pixels with value 1\n",
    "rows, cols = np.where(raster_data == 1)\n",
    "\n",
    "# Step 3: Convert these indices to geographic coordinates\n",
    "points = [Point(transform * (col, row)) for row, col in zip(rows, cols)]\n",
    "\n",
    "# Step 4: Create a GeoDataFrame with these points\n",
    "gdf = gpd.GeoDataFrame(geometry=points, crs=crs)\n",
    "\n",
    "# Step 5: Export the points to a shapefile\n",
    "output_shp_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\COMBINED_30m_points2.shp\"\n",
    "gdf.to_file(output_shp_path)\n",
    "\n",
    "print(f\"Points shapefile saved to: {output_shp_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888f037-c761-4c4f-8064-52118e013cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f473ab98-9381-4c3b-8e6e-b3ba00131a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rasters: 100%|████████████████████████████████████████████████████████████████| 2/2 [01:32<00:00, 46.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted values saved to: C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\extracted_valuesfull.csv\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Progress bar library\n",
    "from rasterio.vrt import WarpedVRT\n",
    "from rasterio import sample\n",
    "\n",
    "def extract_multi_values_to_points(points_shapefile, raster_files, output_csv=None):\n",
    "    # Step 1: Load the point shapefile using GeoPandas\n",
    "    print(\"2\")\n",
    "    points_gdf = gpd.read_file(points_shapefile)\n",
    "    print(\"3\")\n",
    "    # Initialize an empty DataFrame to store the extracted values\n",
    "    extracted_values = pd.DataFrame()\n",
    "\n",
    "    # Add the point geometries to the DataFrame for reference\n",
    "    extracted_values['geometry'] = points_gdf['geometry']\n",
    "    \n",
    "    # Convert point geometries to a list of coordinate tuples (x, y)\n",
    "    point_coords = [(geom.x, geom.y) for geom in points_gdf.geometry]\n",
    "    print(\"4\")\n",
    "    # Loop through each raster file and extract the values at the point locations\n",
    "    for raster_path in tqdm(raster_files, desc=\"Processing Rasters\"):\n",
    "        with rasterio.open(raster_path) as src:\n",
    "            # Handle CRS reprojection if necessary\n",
    "            if points_gdf.crs != src.crs:\n",
    "                vrt_options = {\n",
    "                    'crs': src.crs,\n",
    "                    'resampling': rasterio.enums.Resampling.nearest\n",
    "                }\n",
    "                with WarpedVRT(src, **vrt_options) as vrt:\n",
    "                    values = [val[0] for val in sample.sample_gen(vrt, point_coords)]\n",
    "            else:\n",
    "                # Extract the raster values at all point locations in one go\n",
    "                values = [val[0] for val in sample.sample_gen(src, point_coords)]\n",
    "        \n",
    "        # Get the raster filename to use as a column name\n",
    "        column_name = raster_path.split(\"\\\\\")[-1].replace('.tif', '')\n",
    "        extracted_values[column_name] = values\n",
    "    \n",
    "    # Step 4: Optionally save to CSV\n",
    "    if output_csv:\n",
    "        extracted_values.to_csv(output_csv, index=False)\n",
    "        print(f\"Extracted values saved to: {output_csv}\")\n",
    "    \n",
    "    return extracted_values\n",
    "\n",
    "# Example usage:\n",
    "points_shapefile = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\COMBINED_30m_points.shp\"\n",
    "raster_files = [\n",
    "    r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\Activity_1120_30m_snapped_fy_planned.tif\",\n",
    "    r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\WRC2020\\CO\\BP_CO.tif\"\n",
    "]\n",
    "output_csv = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\extracted_valuesfull.csv\"\n",
    "\n",
    "# Call the function with optimized batch extraction\n",
    "extracted_df = extract_multi_values_to_points(points_shapefile, raster_files, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d37993fb-554d-4892-9c16-91c32921ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1196380\n",
      "First few rows of the DataFrame:\n",
      "                  geometry  Activity_1120_30m_snapped_fy_planned     BP_CO\n",
      "0  POINT (-907965 2051385)                                  2016  0.002075\n",
      "1  POINT (-907935 2051385)                                  2016  0.002080\n",
      "2  POINT (-907905 2051385)                                  2016  0.002084\n",
      "3  POINT (-907875 2051385)                                  2016  0.002087\n",
      "4  POINT (-907965 2051355)                                  2016  0.002068\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path\n",
    "csv_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\extracted_valuesfull.csv\"\n",
    "\n",
    "# Try to read the CSV file into a DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Get the number of rows\n",
    "    num_rows = df.shape[0]\n",
    "    \n",
    "    # Display the number of rows and the first few rows of the DataFrame\n",
    "    print(f\"Number of rows: {num_rows}\")\n",
    "    print(\"First few rows of the DataFrame:\")\n",
    "    print(df.head())  # By default, .head() displays the first 5 rows\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69bd4ea-0aa4-4040-8446-02478781fc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43f4ed8-f551-436d-ab13-2b8dd706e437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f20bd5-42b5-4361-9d09-762521d70d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450099b5-c195-47c3-bfa8-3a87edc72af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df570f8e-5fae-460a-92c7-7a444b0622cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import rasterio\n",
    "# import pandas as pd\n",
    "# from rasterio.transform import xy\n",
    "# from rasterio.vrt import WarpedVRT\n",
    "# from pyproj import CRS\n",
    "# from tqdm import tqdm  # Progress bar library\n",
    "\n",
    "# def extract_multi_values_to_points(points_shapefile, raster_files, output_csv=None):\n",
    "#     # Step 1: Load the point shapefile using GeoPandas\n",
    "#     points_gdf = gpd.read_file(points_shapefile)\n",
    "    \n",
    "#     # Initialize an empty DataFrame to store the extracted values\n",
    "#     extracted_values = pd.DataFrame()\n",
    "\n",
    "#     # Add the point geometries to the DataFrame for reference\n",
    "#     extracted_values['geometry'] = points_gdf['geometry']\n",
    "    \n",
    "#     # Loop through each raster file and extract the values at the point locations\n",
    "#     for raster_path in tqdm(raster_files, desc=\"Processing Rasters\"):\n",
    "#         with rasterio.open(raster_path) as src:\n",
    "#             # Handle CRS reprojection if necessary\n",
    "#             if points_gdf.crs != src.crs:\n",
    "#                 vrt_options = {\n",
    "#                     'crs': src.crs,\n",
    "#                     'resampling': rasterio.enums.Resampling.nearest\n",
    "#                 }\n",
    "#                 with WarpedVRT(src, **vrt_options) as vrt:\n",
    "#                     values = extract_values(vrt, points_gdf)\n",
    "#             else:\n",
    "#                 values = extract_values(src, points_gdf)\n",
    "        \n",
    "#         # Get the raster filename to use as a column name\n",
    "#         column_name = raster_path.split(\"/\")[-1].replace('.tif', '')\n",
    "#         extracted_values[column_name] = values\n",
    "    \n",
    "#     # Step 4: Optionally save to CSV\n",
    "#     if output_csv:\n",
    "#         extracted_values.to_csv(output_csv, index=False)\n",
    "#         print(f\"Extracted values saved to: {output_csv}\")\n",
    "    \n",
    "#     return extracted_values\n",
    "\n",
    "# def extract_values(raster, points_gdf):\n",
    "#     values = []\n",
    "#     # Use tqdm to add progress bar for the point extraction\n",
    "#     for point in tqdm(points_gdf.geometry, desc=\"Extracting Values\"):\n",
    "#         # Get the row/column of the raster pixel containing the point\n",
    "#         row, col = raster.index(point.x, point.y)\n",
    "        \n",
    "#         # Extract the pixel value at the point\n",
    "#         value = raster.read(1)[row, col]  # Reading the first band\n",
    "        \n",
    "#         # Append the extracted value to the list\n",
    "#         values.append(value)\n",
    "    \n",
    "#     return values\n",
    "\n",
    "# # Example usage:\n",
    "# points_shapefile = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\COMBINED_30m_points.shp\"\n",
    "# raster_files = [\n",
    "#     r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\Activity_1120_30m_snapped_fy_planned.tif\",\n",
    "#     r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\WRC2020\\CO\\BP_CO.tif\"\n",
    "# ]\n",
    "# output_csv = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\extracted_values.csv\"\n",
    "\n",
    "# # Call the function with progress indicators\n",
    "# extracted_df = extract_multi_values_to_points(points_shapefile, raster_files, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efdc490-6248-49fb-817f-a58646ac5f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bb1dc-5175-4e31-8574-3c54b0a1f53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e962f-3e28-4b0f-8a3b-4203560229eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024d8e7-0526-4f2b-933d-72bace9bb518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SLOW\n",
    "# import geopandas as gpd\n",
    "# import rasterio\n",
    "# import pandas as pd\n",
    "# from rasterio.warp import transform\n",
    "# from rasterio.features import geometry_mask\n",
    "\n",
    "# # Step 1: Load the point shapefile\n",
    "# #points_gdf_FULL = gpd.read_file(r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\COMBINED_30m_points.shp\")\n",
    "# #points_gdf = points_gdf_FULL.head(10)\n",
    "\n",
    "# points_gdf = gdf\n",
    "\n",
    "# print(\"1\")\n",
    "# # Initialize an empty DataFrame to store results\n",
    "# results_df = pd.DataFrame(points_gdf[['geometry']])  # Start with points geometry\n",
    "\n",
    "# # List of rasters to extract from\n",
    "# raster_files = [\n",
    "#     r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\Activity_1120_30m_snapped_fy_planned.tif\",\n",
    "#     r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\WRC2020\\CO\\BP_CO.tif\"\n",
    "#     # Add other rasters here\n",
    "# ]\n",
    "\n",
    "# # Step 2-4: Loop through each raster\n",
    "# for raster_path in raster_files:\n",
    "#     with rasterio.open(raster_path) as src:\n",
    "#         print(\"!\")\n",
    "#         raster_crs = src.crs  # Get the CRS of the raster\n",
    "        \n",
    "#         # Reproject points to match the raster CRS if necessary\n",
    "#         if points_gdf.crs != raster_crs:\n",
    "#             points_reprojected = points_gdf.to_crs(raster_crs)\n",
    "#         else:\n",
    "#             points_reprojected = points_gdf\n",
    "        \n",
    "#         # Get coordinates of the points\n",
    "#         point_coords = [(geom.x, geom.y) for geom in points_reprojected.geometry]\n",
    "\n",
    "#         # Extract raster values at point coordinates\n",
    "#         raster_values = []\n",
    "#         for coord in point_coords:\n",
    "#             for val in src.sample([coord]):\n",
    "#                 raster_values.append(val[0])  # Extract the first (and typically only) band\n",
    "\n",
    "#         # Add extracted values as a new column in the DataFrame\n",
    "#         column_name = raster_path.split(\"\\\\\")[-1]  # Use the raster filename as the column name\n",
    "#         results_df[column_name] = raster_values\n",
    "\n",
    "# # Step 5: Save the final DataFrame\n",
    "# #output_csv_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\points_raster_values.csv\"\n",
    "# #results_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14c728-da45-49c1-a63a-16871a31e19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cfd3bb-1fa7-4bfa-bbf2-609ca88c8a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rasterio\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # List of raster files\n",
    "# raster_files = [\n",
    "#     r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\Activity_1120_30m_snapped_fy_planned.tif\",\n",
    "#     r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\WRC2020\\CO\\BP_CO.tif\"\n",
    "# ]\n",
    "\n",
    "# # Initialize a list to store data from each raster\n",
    "# raster_data = []\n",
    "\n",
    "# # Loop through each raster and load the data\n",
    "# for raster_path in raster_files:\n",
    "#     print(\"1\")\n",
    "#     with rasterio.open(raster_path) as src:\n",
    "#         print(\"2\")\n",
    "#         # Read the first band (if the raster has multiple bands, modify accordingly)\n",
    "#         data = src.read(1)  # Read the first band\n",
    "#         # Flatten the 2D array into 1D to make it compatible with a DataFrame\n",
    "#         raster_data.append(data.flatten())\n",
    "\n",
    "# # Create a DataFrame from the flattened raster data\n",
    "# df = pd.DataFrame({\n",
    "#     'Activity_1120_30m': raster_data[0],\n",
    "#     'BP_CO': raster_data[1]\n",
    "# })\n",
    "\n",
    "# # Handle nodata values (if any)\n",
    "# df.replace(src.nodata, np.nan, inplace=True)\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# #output_csv_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\combined_rasters.csv\"\n",
    "# #df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# print(f\"DataFrame saved to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac0deb-7b7f-45c6-8e22-d9e102a42f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rasterio\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Define a function to read raster data in chunks\n",
    "# def read_raster_in_chunks(raster_path, chunk_size=1024):\n",
    "#     with rasterio.open(raster_path) as src:\n",
    "#         for ji, window in src.block_windows(1):\n",
    "#             data = src.read(1, window=window)\n",
    "#             # Flatten the array and yield it\n",
    "#             yield data.flatten()\n",
    "\n",
    "# # List of raster files\n",
    "# raster_files = [\n",
    "#     r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\ColoradoHFT\\Activity_1120_30m_snapped_fy_planned.tif\",\n",
    "#     r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\WRC2020\\CO\\BP_CO.tif\"\n",
    "# ]\n",
    "\n",
    "# # Initialize a DataFrame\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# # Read data from each raster and append to the DataFrame\n",
    "# for raster_path in raster_files:\n",
    "#     print(\"1\")\n",
    "#     data_generator = read_raster_in_chunks(raster_path)\n",
    "#     print(\"1\")\n",
    "#     column_name = raster_path.split(\"\\\\\")[-1]  # Extract a name from the path\n",
    "#     # Convert each chunk to a DataFrame and concatenate\n",
    "#     print(\"1\")\n",
    "#     for chunk in data_generator:\n",
    "#         temp_df = pd.DataFrame(chunk, columns=[column_name])\n",
    "#         df = pd.concat([df, temp_df], ignore_index=True)\n",
    "\n",
    "# # Replace nodata values with NaN\n",
    "# nodata = src.nodatavals[0]\n",
    "# df.replace(nodata, np.nan, inplace=True)\n",
    "\n",
    "# # Optionally, save the DataFrame to a CSV file\n",
    "# #output_csv_path = r\"C:\\Users\\magst\\Desktop\\CHAPTER1_DataRerun\\combined_rasters.csv\"\n",
    "# #df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# print(f\"DataFrame saved to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a937e6ed-822a-4da4-be15-a0b8b73cdbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
